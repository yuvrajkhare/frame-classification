# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SRaS0u-ddU9ULvV96WZSop4zXNmk0_YL
"""

import torch
import torch.nn as nn
import timm

class ProjectionHead(nn.Module):
    def __init__(self, in_dim, out_dim, hidden_dim=2048):
        super(ProjectionHead, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim),
        )

    def forward(self, x):
        return nn.functional.normalize(self.mlp(x), dim=-1)

def create_classifier(pretrained_weights_path=None):
    # Create the base model using the same architecture as in pretraining
    base_model = timm.create_model('tf_efficientnetv2_s', pretrained=True)
    num_ftrs = base_model.classifier.in_features
    base_model.classifier = nn.Identity()  # Remove the final classification layer
    
    # Create the ProjectionHead used in the pretraining
    projection_head = ProjectionHead(num_ftrs, 128)  # Match the output dimension used in pretraining
    
    # Combine the base model and projection head
    model = nn.Sequential(
        base_model,
        projection_head
    )
    
    # Load the pretrained contrastive learning weights
    if pretrained_weights_path:
        state_dict = torch.load(pretrained_weights_path, map_location=torch.device('cpu'))
        model.load_state_dict(state_dict)
        print("Pretrained weights loaded successfully.")
    else:
        print("No pretrained weights provided, training from scratch.")

    # Remove the projection head after loading weights (keeping only the backbone)
    base_model = model[0]

    # Define the classifier with a new head for fine-tuning
    classifier = nn.Sequential(
        base_model,
        nn.Flatten(),
        nn.Linear(num_ftrs, 128),
        nn.ReLU(),
        nn.Dropout(p=0.5),
        nn.Linear(128, 3)  # Assuming 3 classes for fine-tuning
    )

    return classifier
